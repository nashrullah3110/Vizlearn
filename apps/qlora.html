<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="UTF-8">
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZT6JM33V5J"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-ZT6JM33V5J');
        </script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Explainer: QLoRA Fine-tuning</title>
    
    <!-- External Dependencies -->
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <style>
        /* Custom theme configuration */
        body {
            font-family: 'Roboto Mono', monospace;
        }
        
        /* Custom scrollbar for dark theme */
        ::-webkit-scrollbar {
            width: 8px;
        }
        
        ::-webkit-scrollbar-track {
            background: #374151;
        }
        
        ::-webkit-scrollbar-thumb {
            background: #8b5cf6;
            border-radius: 4px;
        }
        
        ::-webkit-scrollbar-thumb:hover {
            background: #a78bfa;
        }
        
        /* Button styles */
        .btn-primary {
            background: #8b5cf6;
            transition: all 0.3s ease;
        }
        
        .btn-primary:hover:not(:disabled) {
            background: #a78bfa;
            transform: translateY(-1px);
            box-shadow: 0 4px 15px rgba(139, 92, 246, 0.4);
        }
        
        .btn-primary:active:not(:disabled) {
            transform: scale(0.98);
        }
        
        .btn-primary:disabled {
            background: #374151;
            color: #9ca3af;
            cursor: not-allowed;
            transform: none;
        }
        
        .btn-secondary {
            background: #374151;
            transition: all 0.3s ease;
        }
        
        .btn-secondary:hover:not(:disabled) {
            background: #4b5563;
            transform: translateY(-1px);
        }
        
        .btn-secondary:active:not(:disabled) {
            transform: scale(0.98);
        }
        
        .btn-secondary:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        
        /* Model visualization styles */
        .model-block {
            border: 2px solid #374151;
            border-radius: 16px;
            padding: 24px;
            margin: 16px;
            background: #1f2937;
            position: relative;
            transition: all 0.8s ease;
            min-height: 120px;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
        }
        
        .model-block.large {
            width: 200px;
            height: 200px;
            background: linear-gradient(135deg, #374151, #4b5563);
        }
        
        .model-block.quantized {
            background: linear-gradient(135deg, #1f2937, #374151);
            border-color: #8b5cf6;
            box-shadow: 0 0 20px rgba(139, 92, 246, 0.3);
        }
        
        .model-block.quantized::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: repeating-linear-gradient(
                45deg,
                transparent,
                transparent 8px,
                rgba(139, 92, 246, 0.1) 8px,
                rgba(139, 92, 246, 0.1) 16px
            );
            border-radius: 14px;
            pointer-events: none;
        }
        
        .lora-adapter {
            width: 60px;
            height: 80px;
            background: linear-gradient(135deg, #8b5cf6, #a78bfa);
            border: 2px solid #8b5cf6;
            border-radius: 12px;
            margin: 8px;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            position: relative;
            transition: all 0.5s ease;
            box-shadow: 0 4px 15px rgba(139, 92, 246, 0.4);
        }
        
        .lora-adapter.training {
            animation: training-glow 2s infinite;
        }
        
        @keyframes training-glow {
            0%, 100% { 
                box-shadow: 0 4px 15px rgba(139, 92, 246, 0.4);
                background: linear-gradient(135deg, #8b5cf6, #a78bfa);
            }
            50% { 
                box-shadow: 0 8px 25px rgba(167, 139, 250, 0.8);
                background: linear-gradient(135deg, #a78bfa, #c4b5fd);
            }
        }
        
        .memory-display {
            position: absolute;
            top: -12px;
            right: -12px;
            background: #10b981;
            color: white;
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: bold;
            box-shadow: 0 2px 8px rgba(16, 185, 129, 0.4);
            transition: all 0.5s ease;
        }
        
        .memory-display.reduced {
            background: #f59e0b;
        }
        
        .memory-display.minimal {
            background: #8b5cf6;
        }
        
        /* Animation classes */
        .fade-in {
            animation: fadeIn 1s ease-in-out forwards;
        }
        
        @keyframes fadeIn {
            0% { opacity: 0; transform: translateY(20px); }
            100% { opacity: 1; transform: translateY(0); }
        }
        
        .slide-in {
            animation: slideIn 0.8s ease-out forwards;
        }
        
        @keyframes slideIn {
            0% { opacity: 0; transform: translateX(-50px); }
            100% { opacity: 1; transform: translateX(0); }
        }
        
        .transform-effect {
            animation: transform 1.5s ease-in-out forwards;
        }
        
        @keyframes transform {
            0% { transform: scale(1); }
            50% { transform: scale(1.1) rotateY(180deg); }
            100% { transform: scale(1) rotateY(360deg); }
        }
        
        .connect-animation {
            animation: connect 1s ease-in-out forwards;
        }
        
        @keyframes connect {
            0% { opacity: 0; transform: scale(0.5) rotate(-45deg); }
            100% { opacity: 1; transform: scale(1) rotate(0deg); }
        }
        
        /* Step indicator */
        .step-indicator {
            display: flex;
            align-items: center;
            margin-bottom: 24px;
            padding: 16px;
            background: #101827;
            border: 1px solid #374151;
            border-radius: 12px;
        }
        
        .step-number {
            width: 32px;
            height: 32px;
            border-radius: 50%;
            background: #8b5cf6;
            color: white;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            margin-right: 16px;
            font-size: 14px;
        }
        
        .step-number.inactive {
            background: #374151;
            color: #9ca3af;
        }
        
        .step-title {
            font-weight: bold;
            font-size: 16px;
            color: #f3f4f6;
        }
        
        /* Visualization container */
        .visualization-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            min-height: 400px;
            position: relative;
        }
        
        .model-row {
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 20px 0;
            flex-wrap: wrap;
            gap: 16px;
        }
        
        .connection-line {
            width: 2px;
            height: 30px;
            background: #8b5cf6;
            margin: 0 16px;
            opacity: 0;
            transition: opacity 0.5s ease;
        }
        
        .connection-line.active {
            opacity: 1;
        }
        
        /* Training status */
        .training-status {
            position: absolute;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: #f59e0b;
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 12px;
            font-weight: bold;
            opacity: 0;
            transition: opacity 0.5s ease;
        }
        
        .training-status.active {
            opacity: 1;
            animation: pulse 2s infinite;
        }
        
        @keyframes pulse {
            0%, 100% { transform: translateX(-50%) scale(1); }
            50% { transform: translateX(-50%) scale(1.05); }
        }
        
        /* Responsive adjustments */
        @media (max-width: 768px) {
            .model-block.large {
                width: 150px;
                height: 150px;
            }
            
            .lora-adapter {
                width: 50px;
                height: 70px;
            }
            
            .model-row {
                flex-direction: column;
            }
            
            .connection-line {
                width: 30px;
                height: 2px;
                transform: rotate(90deg);
            }
        }
        
        /* Welcome state */
        .welcome-message {
            text-align: center;
            padding: 40px;
            color: #9ca3af;
        }
        
        .welcome-message h3 {
            color: #f3f4f6;
            margin-bottom: 16px;
            font-size: 18px;
        }
    </style>
</head>
<body class="bg-[#101827] text-[#f3f4f6] flex flex-col min-h-screen">
    <!-- Header -->
    <header class="py-6 px-4 text-center border-b border-[#374151]">
        <h1 class="text-3xl md:text-4xl font-bold text-[#f3f4f6]">
            LLM Finetuning with QLoRA
        </h1>
    </header>

    <!-- Main Content -->
    <main class="flex-grow flex flex-col lg:flex-row gap-6 p-6">
        <!-- Left Panel: Controls & Explanation (40% width) -->
        <div class="lg:w-2/5 bg-[#1f2937] rounded-xl p-6 border border-[#374151]">
            <div class="h-full flex flex-col">
                <!-- Step Indicator -->
                <div id="stepIndicator" class="step-indicator">
                    <div id="stepNumber" class="step-number">0</div>
                    <div>
                        <div id="stepTitle" class="step-title">Welcome to QLoRA</div>
                    </div>
                </div>

                <!-- Explanation Content -->
                <div class="flex-grow">
                    <div id="explanationContent" class="text-sm text-[#9ca3af] leading-relaxed space-y-4">
                        <p>
                            <strong class="text-[#f3f4f6]">QLoRA (Quantized Low-Rank Adaptation)</strong> enables efficient finetuning of large language models on consumer-grade hardware.
                        </p>
                        <p>
                            The challenge: Finetuning large models like Llama 2 7B requires enormous memory and compute resources, making it inaccessible to most practitioners.
                        </p>
                        <p>
                            The solution: QLoRA combines <strong class="text-[#8b5cf6]">4-bit quantization</strong> with <strong class="text-[#8b5cf6]">LoRA adapters</strong> to dramatically reduce memory requirements while maintaining training effectiveness.
                        </p>
                        <p class="text-[#a78bfa]">
                            Click "Begin" to start the interactive demonstration.
                        </p>
                    </div>
                </div>

                <!-- Controls -->
                <div class="mt-6 space-y-4">
                    <button 
                        id="nextStepBtn" 
                        class="btn-primary w-full py-3 px-4 rounded-lg font-medium text-white flex items-center justify-center"
                    >
                        <i class="fas fa-play mr-2"></i>
                        Begin
                    </button>
                    
                    <button 
                        id="resetBtn" 
                        class="btn-secondary w-full py-3 px-4 rounded-lg font-medium text-[#f3f4f6] flex items-center justify-center"
                    >
                        <i class="fas fa-undo mr-2"></i>
                        Reset
                    </button>
                </div>

                <!-- Memory Summary -->
                <div id="memorySummary" class="mt-6 bg-[#101827] border border-[#374151] rounded-lg p-4" style="display: none;">
                    <h4 class="text-sm font-semibold text-[#a78bfa] mb-3">
                        <i class="fas fa-microchip mr-1"></i>
                        Memory Usage Summary
                    </h4>
                    <div class="space-y-2 text-xs">
                        <div class="flex justify-between">
                            <span class="text-[#9ca3af]">Original Model (FP16):</span>
                            <span class="text-[#f59e0b]">~14 GB</span>
                        </div>
                        <div class="flex justify-between">
                            <span class="text-[#9ca3af]">Quantized Model (NF4):</span>
                            <span class="text-[#10b981]">~3.5 GB</span>
                        </div>
                        <div class="flex justify-between">
                            <span class="text-[#9ca3af]">LoRA Adapters:</span>
                            <span class="text-[#8b5cf6]">~0.2 GB</span>
                        </div>
                        <div class="border-t border-[#374151] pt-2 flex justify-between font-semibold">
                            <span class="text-[#f3f4f6]">Total QLoRA:</span>
                            <span class="text-[#10b981]">~3.7 GB</span>
                        </div>
                        <div class="text-center text-[#10b981] font-semibold">
                            75% Memory Reduction!
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Right Panel: Core Visualization (60% width) -->
        <div class="lg:w-3/5 bg-[#1f2937] rounded-xl p-6 border border-[#374151]">
            <div class="h-full flex flex-col">
                <!-- Visualization Header -->
                <div class="flex items-center justify-between mb-6">
                    <h2 class="text-xl font-semibold text-[#f3f4f6]">
                        <i class="fas fa-cogs text-[#8b5cf6] mr-2"></i>
                        QLoRA Process Visualization
                    </h2>
                    <div class="flex items-center space-x-4 text-sm">
                        <div class="flex items-center">
                            <div class="w-3 h-3 bg-[#4b5563] rounded mr-2"></div>
                            <span class="text-[#9ca3af]">Frozen</span>
                        </div>
                        <div class="flex items-center">
                            <div class="w-3 h-3 bg-[#8b5cf6] rounded mr-2"></div>
                            <span class="text-[#9ca3af]">Trainable</span>
                        </div>
                    </div>
                </div>

                <!-- Main Visualization Area -->
                <div class="flex-grow">
                    <div id="visualizationContainer" class="visualization-container">
                        <!-- Welcome State -->
                        <div id="welcomeState" class="welcome-message">
                            <h3>Ready to Explore QLoRA</h3>
                            <p>Click "Begin" to start the step-by-step demonstration of how QLoRA enables efficient LLM finetuning.</p>
                            <div class="mt-4">
                                <i class="fas fa-arrow-left text-[#8b5cf6] text-2xl"></i>
                            </div>
                        </div>

                        <!-- Step 1: Base Model -->
                        <div id="step1Visualization" style="display: none;">
                            <div class="model-row">
                                <div id="baseModel" class="model-block large">
                                    <div class="memory-display">~14 GB</div>
                                    <div class="text-center">
                                        <div class="text-[#f3f4f6] font-bold text-sm mb-2">Base LLM</div>
                                        <div class="text-[#9ca3af] text-xs">FP16 Weights</div>
                                        <div class="text-[#9ca3af] text-xs">Llama 2 7B</div>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <!-- Step 2: Quantized Model -->
                        <div id="step2Visualization" style="display: none;">
                            <div class="model-row">
                                <div id="quantizedModel" class="model-block large quantized">
                                    <div class="memory-display reduced">~3.5 GB</div>
                                    <div class="text-center">
                                        <div class="text-[#f3f4f6] font-bold text-sm mb-2">Quantized LLM</div>
                                        <div class="text-[#8b5cf6] text-xs font-semibold">Frozen NF4 Weights</div>
                                        <div class="text-[#9ca3af] text-xs">4-bit Precision</div>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <!-- Step 3: LoRA Adapters -->
                        <div id="step3Visualization" style="display: none;">
                            <div class="model-row">
                                <div class="model-block large quantized">
                                    <div class="memory-display reduced">~3.5 GB</div>
                                    <div class="text-center">
                                        <div class="text-[#f3f4f6] font-bold text-sm mb-2">Quantized LLM</div>
                                        <div class="text-[#8b5cf6] text-xs font-semibold">Frozen NF4 Weights</div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="connection-line active"></div>
                            
                            <div class="model-row">
                                <div id="loraAdapter1" class="lora-adapter">
                                    <div class="memory-display minimal">~0.1 GB</div>
                                    <div class="text-center">
                                        <div class="text-white text-xs font-bold">LoRA A</div>
                                        <div class="text-white text-xs">Trainable</div>
                                    </div>
                                </div>
                                <div id="loraAdapter2" class="lora-adapter">
                                    <div class="memory-display minimal">~0.1 GB</div>
                                    <div class="text-center">
                                        <div class="text-white text-xs font-bold">LoRA B</div>
                                        <div class="text-white text-xs">Trainable</div>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <!-- Step 4: Training -->
                        <div id="step4Visualization" style="display: none;">
                            <div class="model-row">
                                <div class="model-block large quantized" style="opacity: 0.6;">
                                    <div class="memory-display reduced">~3.5 GB</div>
                                    <div class="text-center">
                                        <div class="text-[#f3f4f6] font-bold text-sm mb-2">Quantized LLM</div>
                                        <div class="text-[#8b5cf6] text-xs font-semibold">Frozen NF4 Weights</div>
                                        <div class="text-[#9ca3af] text-xs">Unchanged</div>
                                    </div>
                                </div>
                            </div>
                            
                            <div class="connection-line active"></div>
                            
                            <div class="model-row">
                                <div class="lora-adapter training">
                                    <div class="memory-display minimal">~0.1 GB</div>
                                    <div class="text-center">
                                        <div class="text-white text-xs font-bold">LoRA A</div>
                                        <div class="text-white text-xs">Training</div>
                                    </div>
                                </div>
                                <div class="lora-adapter training">
                                    <div class="memory-display minimal">~0.1 GB</div>
                                    <div class="text-center">
                                        <div class="text-white text-xs font-bold">LoRA B</div>
                                        <div class="text-white text-xs">Training</div>
                                    </div>
                                </div>
                            </div>
                            
                            <div id="trainingStatus" class="training-status active">
                                <i class="fas fa-spinner fa-spin mr-2"></i>
                                Training in Progress...
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Key Benefits -->
                <div class="mt-6 bg-[#101827] border border-[#374151] rounded-lg p-4">
                    <h4 class="text-sm font-semibold text-[#a78bfa] mb-3">
                        <i class="fas fa-star mr-1"></i>
                        QLoRA Advantages
                    </h4>
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-3 text-xs">
                        <div class="text-[#9ca3af]">
                            <strong class="text-[#10b981]">Memory Efficient:</strong> 4-bit quantization reduces memory by ~75%
                        </div>
                        <div class="text-[#9ca3af]">
                            <strong class="text-[#10b981]">Selective Training:</strong> Only LoRA adapters are updated
                        </div>
                        <div class="text-[#9ca3af]">
                            <strong class="text-[#10b981]">Consumer Hardware:</strong> Enables training on single GPU
                        </div>
                        <div class="text-[#9ca3af]">
                            <strong class="text-[#10b981]">Performance Maintained:</strong> Minimal quality degradation
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </main>

    <!-- Footer -->
    <footer class="bg-[#1f2937] border-t border-[#374151] py-6 mt-auto">
        <div class="flex justify-center space-x-6">
            <a href="https://www.linkedin.com/in/ashish-jangra/" target="_blank" rel="noopener noreferrer" 
               class="text-[#9ca3af] hover:text-[#a78bfa] transition-colors duration-300">
                <i class="fab fa-linkedin fa-2x"></i>
            </a>
            <a href="https://github.com/AshishJangra27" target="_blank" rel="noopener noreferrer" 
               class="text-[#9ca3af] hover:text-[#a78bfa] transition-colors duration-300">
                <i class="fab fa-github fa-2x"></i>
            </a>
            <a href="https://www.kaggle.com/ashishjangra27" target="_blank" rel="noopener noreferrer" 
               class="text-[#9ca3af] hover:text-[#a78bfa] transition-colors duration-300">
                <i class="fab fa-kaggle fa-2x"></i>
            </a>
            <a href="https://huggingface.co/ashish-jangra" target="_blank" rel="noopener noreferrer" 
               class="text-[#9ca3af] hover:text-[#a78bfa] transition-colors duration-300">
                <svg width="32" height="32" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M12.036 3L4.5 7.73v8.54l7.536 4.73 7.536-4.73V7.73L12.036 3zm5.157 3.74L12 10.5 6.807 6.74 12 4.73l5.193 2.01zm-5.157 12.72l-5.036-3.16V9.04L12 13.08l5.036-4.04v7.26L12.036 19.46z"/>
                </svg>
            </a>
            <a href="https://www.instagram.com/ashish_zangra/" target="_blank" rel="noopener noreferrer" 
               class="text-[#9ca3af] hover:text-[#a78bfa] transition-colors duration-300">
                <i class="fab fa-instagram fa-2x"></i>
            </a>
        </div>
    </footer>

    <script>
        // Global state management
        let currentStep = 0;
        const maxSteps = 4;
        let isAnimating = false;

        // DOM elements
        const nextStepBtn = document.getElementById('nextStepBtn');
        const resetBtn = document.getElementById('resetBtn');
        const stepNumber = document.getElementById('stepNumber');
        const stepTitle = document.getElementById('stepTitle');
        const explanationContent = document.getElementById('explanationContent');
        const memorySummary = document.getElementById('memorySummary');

        // Visualization elements
        const welcomeState = document.getElementById('welcomeState');
        const step1Visualization = document.getElementById('step1Visualization');
        const step2Visualization = document.getElementById('step2Visualization');
        const step3Visualization = document.getElementById('step3Visualization');
        const step4Visualization = document.getElementById('step4Visualization');

        // Step content configuration
        const stepConfig = {
            0: {
                title: "Welcome to QLoRA",
                button: "Begin",
                icon: "fas fa-play",
                content: `
                    <p>
                        <strong class="text-[#f3f4f6]">QLoRA (Quantized Low-Rank Adaptation)</strong> enables efficient finetuning of large language models on consumer-grade hardware.
                    </p>
                    <p>
                        The challenge: Finetuning large models like Llama 2 7B requires enormous memory and compute resources, making it inaccessible to most practitioners.
                    </p>
                    <p>
                        The solution: QLoRA combines <strong class="text-[#8b5cf6]">4-bit quantization</strong> with <strong class="text-[#8b5cf6]">LoRA adapters</strong> to dramatically reduce memory requirements while maintaining training effectiveness.
                    </p>
                    <p class="text-[#a78bfa]">
                        Click "Begin" to start the interactive demonstration.
                    </p>
                `
            },
            1: {
                title: "The Base Model",
                button: "Next Step",
                icon: "fas fa-arrow-right",
                content: `
                    <p>
                        <strong class="text-[#f3f4f6]">Step 1: Base Large Language Model</strong>
                    </p>
                    <p>
                        We start with a large pre-trained model like <strong class="text-[#8b5cf6]">Llama 2 7B</strong> with weights stored in 16-bit floating point precision (FP16).
                    </p>
                    <p>
                        <strong class="text-[#f59e0b]">Memory Challenge:</strong> This model requires approximately <strong class="text-[#f59e0b]">14 GB of VRAM</strong> just to load, making it impossible to fit on most consumer GPUs.
                    </p>
                    <p>
                        Traditional finetuning would require even more memory for gradients and optimizer states, often exceeding 40+ GB.
                    </p>
                `
            },
            2: {
                title: "4-bit Quantization",
                button: "Next Step",
                icon: "fas fa-arrow-right",
                content: `
                    <p>
                        <strong class="text-[#f3f4f6]">Step 2: 4-bit Quantization</strong>
                    </p>
                    <p>
                        QLoRA applies <strong class="text-[#8b5cf6]">4-bit NormalFloat (NF4) quantization</strong> to compress the model weights from 16-bit to 4-bit precision.
                    </p>
                    <p>
                        <strong class="text-[#10b981]">Memory Reduction:</strong> This reduces memory usage from ~14 GB to just <strong class="text-[#10b981]">~3.5 GB</strong> - a 75% reduction!
                    </p>
                    <p>
                        The quantized weights are <strong class="text-[#8b5cf6]">frozen</strong> during training, preserving the pre-trained knowledge while enabling memory-efficient inference.
                    </p>
                `
            },
            3: {
                title: "Adding LoRA Adapters",
                button: "Next Step",
                icon: "fas fa-arrow-right",
                content: `
                    <p>
                        <strong class="text-[#f3f4f6]">Step 3: LoRA Adapter Integration</strong>
                    </p>
                    <p>
                        Instead of finetuning the entire model, QLoRA introduces small <strong class="text-[#8b5cf6]">Low-Rank Adaptation (LoRA)</strong> layers.
                    </p>
                    <p>
                        These lightweight adapters contain only <strong class="text-[#8b5cf6]">~0.2 GB</strong> of trainable parameters - less than 1% of the original model size.
                    </p>
                    <p>
                        <strong class="text-[#a78bfa]">Key Insight:</strong> By training only these small adapters, we can achieve effective task adaptation without modifying the frozen base model.
                    </p>
                `
            },
            4: {
                title: "Finetuning Process",
                button: "Complete",
                icon: "fas fa-check",
                content: `
                    <p>
                        <strong class="text-[#f3f4f6]">Step 4: Efficient Finetuning</strong>
                    </p>
                    <p>
                        During training, only the <strong class="text-[#8b5cf6]">LoRA adapters are updated</strong> while the quantized base model remains completely frozen.
                    </p>
                    <p>
                        <strong class="text-[#10b981]">Total Memory:</strong> ~3.7 GB (quantized model + LoRA adapters)
                    </p>
                    <p>
                        This represents a <strong class="text-[#10b981]">75% memory reduction</strong> compared to traditional finetuning, enabling LLM training on consumer hardware like RTX 3080/4080 GPUs.
                    </p>
                    <p class="text-[#a78bfa]">
                        QLoRA democratizes LLM finetuning for everyone! ðŸš€
                    </p>
                `
            }
        };

        // Initialize the application
        function init() {
            setupEventListeners();
            updateStepDisplay();
        }

        // Setup event listeners
        function setupEventListeners() {
            nextStepBtn.addEventListener('click', handleNextStep);
            resetBtn.addEventListener('click', handleReset);
        }

        // Handle next step button click
        async function handleNextStep() {
            if (isAnimating) return;
            
            if (currentStep < maxSteps) {
                isAnimating = true;
                nextStepBtn.disabled = true;
                
                currentStep++;
                await updateStepDisplay();
                await playStepAnimation();
                
                nextStepBtn.disabled = false;
                isAnimating = false;
            }
        }

        // Handle reset button click
        function handleReset() {
            if (isAnimating) return;
            
            currentStep = 0;
            hideAllVisualizations();
            welcomeState.style.display = 'block';
            memorySummary.style.display = 'none';
            updateStepDisplay();
        }

        // Update step display (indicator, title, content, button)
        async function updateStepDisplay() {
            const config = stepConfig[currentStep];
            
            // Update step indicator
            stepNumber.textContent = currentStep;
            stepNumber.className = currentStep === 0 ? 'step-number inactive' : 'step-number';
            
            // Update step title
            stepTitle.textContent = config.title;
            
            // Update explanation content with fade effect
            explanationContent.style.opacity = '0';
            await new Promise(resolve => setTimeout(resolve, 200));
            
            explanationContent.innerHTML = config.content;
            explanationContent.style.opacity = '1';
            
            // Update button
            nextStepBtn.innerHTML = `<i class="${config.icon} mr-2"></i>${config.button}`;
            
            // Show memory summary after step 2
            if (currentStep >= 2) {
                memorySummary.style.display = 'block';
                memorySummary.classList.add('fade-in');
            }
            
            // Disable next button if at final step
            if (currentStep >= maxSteps) {
                nextStepBtn.disabled = true;
                nextStepBtn.style.opacity = '0.5';
            }
        }

        // Play step-specific animations
        async function playStepAnimation() {
            hideAllVisualizations();
            
            switch (currentStep) {
                case 1:
                    await animateStep1();
                    break;
                case 2:
                    await animateStep2();
                    break;
                case 3:
                    await animateStep3();
                    break;
                case 4:
                    await animateStep4();
                    break;
            }
        }

        // Step 1: Show base model
        async function animateStep1() {
            step1Visualization.style.display = 'block';
            const baseModel = document.getElementById('baseModel');
            baseModel.classList.add('fade-in');
            await new Promise(resolve => setTimeout(resolve, 1000));
        }

        // Step 2: Transform to quantized model
        async function animateStep2() {
            step2Visualization.style.display = 'block';
            const quantizedModel = document.getElementById('quantizedModel');
            quantizedModel.classList.add('transform-effect');
            await new Promise(resolve => setTimeout(resolve, 1500));
        }

        // Step 3: Add LoRA adapters
        async function animateStep3() {
            step3Visualization.style.display = 'block';
            
            // Show quantized model first
            await new Promise(resolve => setTimeout(resolve, 300));
            
            // Animate LoRA adapters
            const adapter1 = document.getElementById('loraAdapter1');
            const adapter2 = document.getElementById('loraAdapter2');
            
            adapter1.classList.add('connect-animation');
            await new Promise(resolve => setTimeout(resolve, 400));
            
            adapter2.classList.add('connect-animation');
            await new Promise(resolve => setTimeout(resolve, 600));
        }

        // Step 4: Show training process
        async function animateStep4() {
            step4Visualization.style.display = 'block';
            await new Promise(resolve => setTimeout(resolve, 500));
        }

        // Hide all visualization states
        function hideAllVisualizations() {
            welcomeState.style.display = 'none';
            step1Visualization.style.display = 'none';
            step2Visualization.style.display = 'none';
            step3Visualization.style.display = 'none';
            step4Visualization.style.display = 'none';
            
            // Remove animation classes
            document.querySelectorAll('.fade-in, .transform-effect, .connect-animation').forEach(element => {
                element.classList.remove('fade-in', 'transform-effect', 'connect-animation');
            });
        }

        // Initialize when DOM is loaded
        document.addEventListener('DOMContentLoaded', init);
    </script>
</body>
</html>
